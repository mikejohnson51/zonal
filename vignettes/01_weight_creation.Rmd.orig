---
title: "Weight Maps"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Weight Maps}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(dplyr)
library(sf)
library(terra)
library(ggplot2)

knitr::opts_chunk$set(
  out.width = "100%",
  fig.width = 7, 
  fig.height = 4, dpi = 150, fig.path = "w-",
  message = FALSE, warning = FALSE, error = FALSE
)
```

## Methods 

The first step in computing zonal statistics are the need to compute a weight map that can be used to reallocate the gridded data with respect to the percent overlapping each cell. There are two primary packages that tackle this which include `intersectr` (which uses `areal` as a back-end) and `zonal` which uses `exactextractr`. 

### Option 1: Intersectr: 

Setting up the data for a `intersectr` weights map requires (1) computing a vector representation of the grid and (2) intersecting this grid with the aggregation units  to determine the percent overlap or "coverage fraction". Below the workflow from the `intersectr` docs is wrapped in a function that requires a NetCDF file path, a `sf` geometry set, an ID variable from the geometries, and the variable to extract from the grid.

```{r}
library(intersectr)
library(ncmeta)
library(RNetCDF)

intersectr_weights = function(file, geom, ID, var){
  nc_coord_vars <- nc_coord_var(file)
  variable_name <- var
  nc_coord_vars <- filter(nc_coord_vars, variable == variable_name)
  
  nc       <- open.nc(file)
  X_coords <- var.get.nc(nc, nc_coord_vars$X, unpack = TRUE)
  Y_coords <- var.get.nc(nc, nc_coord_vars$Y, unpack = TRUE)
  
  nc_prj <- nc_gm_to_prj(nc_grid_mapping_atts(file))
    
  cell_geometry = create_cell_geometry(X_coords = X_coords,
                         Y_coords = Y_coords,
                         prj = nc_prj,
                         geom = geom, 
                         buffer_dist = 0.1, # Degrees
                         regularize = TRUE)
    
  data_source_cells <- st_sf(dplyr::select(cell_geometry, grid_ids))
  target_polygons   <- st_sf(dplyr::select(geom, !!ID))
  st_agr(data_source_cells) <- "constant"
  st_agr(target_polygons)   <- "constant"
    
  calculate_area_intersection_weights(
      data_source_cells,
      target_polygons, allow_lonlat = TRUE)
}

```

### Option 2: Zonal

In `zonal` grid weights are calculated using `exactextractr` as the back-end. The key function needed is `weighting_grid`.

```{r}
library(zonal)
```

## Use Cases

Here two motivating use cases are shown to compare the efficiency of these two approaches. The first covers a large area but has many small aggregation units. The second also covers a large area but has a few large polygon aggregation units. Each of these poses a unique set of demands with respect to how the calculation is performed. 

## Grid
The gridded data and aggregate units we are working with can be seen below:

```{r}
file = '/Users/mjohnson/Downloads/pet_1979.nc'
(s = terra::rast(file))
```

Looking at the grid we can see in consists of `r terra::ncell(s)` grid cells each with a `r terra::res(s)[1]` meter by `r terra::res(s)[2]` meter resolution. Additionally, there are `r terra::nlyr(s)` unique time slices in the NetCDF file.

## Example 01: Many small aggregation units

Here, we look at an example with ~20,000 watersheds along the east coast.

```{r}
geom <- st_make_valid(read_sf('/Users/mjohnson/github/hydrofabric/workflow/nhd_workflows/cache/ngen_01a-4.gpkg', "catchments"))

glimpse(geom)
```
  
In total we have `r prettyNum(nrow(geom), big.mark = ",")` aggregation units to summarize over the `r prettyNum(terra::nlyr(s))` time steps.

```{r}
bnch <- bench::mark(
  iterations = 1, check = FALSE, time_unit = "s",
  intersectr = intersectr_weights(file, geom, "comid", "potential_evapotranspiration"),
  zonal      = weighting_grid(s, geom, "comid")
)
```

```{r huc01-test, echo = FALSE}
oo = bnch %>%
  dplyr::select(exp = expression, medianTime = median, memoryAllocated = mem_alloc) %>% 
  mutate(exp = names(exp), memoryAllocated = unclass(memoryAllocated)/1e9) %>% 
  tidyr::pivot_longer(-exp) %>% 
  mutate(labs = case_when(name == 'medianTime' ~ "1. Median Time\n(seconds)",
                          name == 'memoryAllocated' ~ "3. Memory\n(GB)",
                          name == "TotalTime" ~ "2. Total Time (precompute weights)\n(sec)"))

ggplot(data = oo) +
  aes(x = exp, y = value) + 
  geom_col(aes(fill = exp)) +
  facet_wrap(~labs, scales = "free_y") + 
  theme_bw() + 
  labs(y = "", x = "", title = "HUC01 Test Case") + 
  theme(legend.position = "bottom") + 
   theme(axis.title.x=element_blank(),
        axis.text.x=element_blank())
```

## Example 02: Few, large aggregation units

Here, we test aggregation the 4km gridded data to 64 counties in Colorado.

```{r}
colorado  = AOI::aoi_get(state = "CO", county = "all")
glimpse(colorado)
```
  
In total we have `r prettyNum(nrow(colorado), big.mark = ",")` aggregation units to summarize over the `r terra::nlyr(s)` time steps.

```{r}
bnch2 <- bench::mark(
  iterations = 1, check = FALSE, time_unit = "s",
  intersectr = intersectr_weights(file, colorado, "name", "potential_evapotranspiration"),
  zonal      = weighting_grid(file, colorado, "name")
)
```

```{r colorado-test, echo = FALSE}
oo2 = bnch2 %>%
  dplyr::select(exp = expression, medianTime = median, memoryAllocated = mem_alloc) %>% 
  mutate(exp = names(exp), memoryAllocated = unclass(memoryAllocated)/1e9) %>% 
  tidyr::pivot_longer(-exp) %>% 
  mutate(labs = case_when(name == 'medianTime' ~ "1. Median Time\n(seconds)",
                          name == 'memoryAllocated' ~ "3. Memory\n(GB)",
                          name == "TotalTime" ~ "2. Total Time (precompute weights)\n(sec)"))

ggplot(data = oo2) +
  aes(x = exp, y = value) + 
  geom_col(aes(fill = exp)) +
  facet_wrap(~labs, scales = "free_y") + 
  theme_bw() + 
  labs(y = "", x = "", title = "Colorado Test Case") + 
  theme(legend.position = "bottom") + 
   theme(axis.title.x=element_blank(),
        axis.text.x=element_blank())
```

Notably the `intersectr` approach, based on vector intersections, is unable to scale well when the aggregation units are large.

## So, what is a weight grid? 

A weight grid is unique to the aggregate units and grid its build from. It contains columns documenting the X and Y indexes of each grid cell along with the grid id. The X,Y are realtive to the entire grid, while the grid ID is relative to subsetwithin the bounding domain of the aggregations unit(s). Additionally the `w` stores the percent overlap between the grid cell and the aggregation unit identified by the ID column which is speified in the `build`weighting_grid` function. An example is shown below:

```{r}
zonal      = zonal::weighting_grid(file, colorado, "name")
head(zonal)
```
